{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "spark: apache spark is a open-source unified analytics engine for large scale data processing. \n",
    "\n",
    "Spark Architecture\n",
    "------------------\n",
    "- Liabrary Layer - spark sql,spark streaming, spark Mllib, spark graphx\n",
    "- Api Layer - Python, Java, Scala, R\n",
    "- Spark Core Layer - Ye provide karta hai RDD API for distributed data processing, task schedular for parallelism, aur an in-memory memory management system.\n",
    "- Cluster Manager - mesos, yarn, kubernets, standalone.\n",
    "- Storage Layer - HDFS, S3, ADLS GEN2, Local Storage"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Spark Execution Model:\n",
    "1. Driver - brain of spark framework\n",
    "2. Executor - worker node responsible for executing the given task.\n",
    "\n",
    "Driver Works: sparkcontext/session initialization, compute the application requirement resoures and also manage dynamic allocation lifecycle, \n",
    "works/Tasks distributor, react on node/executor failure, progress/monitor the executor, send response to user,through the metadata job it powers the spark \n",
    "WEBUI\n",
    "\n",
    "Executor Works: execute the task assigned by driver, report the progress back to driver\n",
    "-----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "Spark Programming Model:\n",
    "1. Job - an action encountered in the application\n",
    "2. Stage - Jobs are divide into stages or A suffle dependency in the application\n",
    "3. Task - smaller unit of execution because one task is launched per partition\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Spark architecture and it's working:\n",
    "\n",
    "Architecture:\n",
    "1. driver runs the main function and creates the sparkcontext/sparksession(bridge between driver and cluster manager), which connects cluster manager and \n",
    "   co-ordinate the execution of the spark application.\n",
    "2. Executors are launched on worker nodes and communicate with driver program and cluster manager for executing task in spark application.\n",
    "3. cluster manager responsible for allocating resources and managing the cluster where spark application runs.\n",
    "\n",
    "Working:\n",
    "SparkSubmit=====> Driver Program launched ======> request resources to cluster manager ======>  main program of user function of the user processing program created\n",
    "\n",
    "Based on main program execution logic processed =====> parallely spark/session also created ====> using spark context different Transformation and action are processed\n",
    "till action called all sparkcontext will go in the form of dag and will create RDD lineage.\n",
    "\n",
    "Once Action called ===> Job created ===> breakdown to stages =====> then to tasks ====> after that tasks are launched by cluster manager on the worker node and\n",
    "this done with the help task Scheduler class.\n",
    "\n",
    "tasks ====> launched to different executors in worker node through cluster for execution.===> resources allocation and tracking of the jobs and task performed\n",
    "by cluster manager.\n",
    "\n",
    "Result return to driver program by executor."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Two Main Abstraction of Spark:\n",
    "\n",
    "1.RDD(Low) - Fundamental datastructure of spark, it is immutable distributed collection of object that can be any type.\n",
    " Spark Dataframe(High) - distributed data structure which store data in two-dimensional table with named columns and defined schema(column name, datatype).\n",
    " Spark Dataset(High) - spark rdd + dataset uses all the capabilities of both (type-safety -> through error on compilation time).\n",
    "\n",
    "Difference:\n",
    "-----------\n",
    "1. RDD - No schema | Dataset, Dataframe - schema\n",
    "2. RDD - slow on JVM launguages | Dataframe, dataset - fast\n",
    "3. RDD - No execution optimization | Dataframe, dataset - catalyst optimizer\n",
    "4. RDD - Low Level | Dataframe, dataset - High\n",
    "5. RDD - No SQL support | dataset, Dataframe - SQL support\n",
    "6. RDD - Type safe | Dataframe - No Type Safe | Dataset - Type Safe\n",
    "7. RDD - Analysis error detect compile time | Dataframe - Runtime | Dataset - compile time\n",
    "8. RDD - High memory used | Dataframe - High | Dataset - Low because of Tungsten\n",
    "\n",
    "------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "Dataframe Vs Dataset:\n",
    "--------------------\n",
    "Data Type: Dataframe is tabular data structure with rows and columns | Dataset is collection of strongly typed JVM objects and it is type-safe.\n",
    "performance: Dataframe is faster than dataset because of dataset used JVM and Dataframe use code generation and it is build on top of rdd and optimized for performance.\n",
    "API: Dataframe have wide variety of APIs and more flexible when it comes to data manipulation | Datset have limited APIs but more concise and expressive.\n",
    "Type Safety: Dataframe Runtime errors | Dataset compile time errors\n",
    "Memory Management: Dataframe use high memory | dataset use low memory because of Tungsten \n",
    "launguages: Java, Python, Scala, R | Scala, Java\n",
    "\n",
    "Dataset preferred over Dataframe when data is strongly typed i.e when the schema is known ahead of time and the data is not necessarily homogeneous.\n",
    "In addition, dataset can take advantages of catalyst optimizer for more efficient execution."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "2.DAG: DAG is a more general concept that represents the logical execution plan for a set of stages in a Spark job. \n",
    "       It is a combination of vertices as well as edges where vertices represents RDDs and edges represents operation to be applied on RDD.\n",
    "\n",
    "Components: Stages | DAG Scheduler\n",
    "\n",
    "Stages: set of tasks execute together in a single wave of computation.\n",
    "DAG Scheduler: High - level scheduling layer implements stage-oriented scheduling.\n",
    "Work: 1. computes the stages of each job and schdule it, 2.subimt Task set to TaskScheduler 3. convert Logical Execution Plan to Physical Execution Plan.\n",
    "4. React on fault tolerance\n",
    "\n",
    "\n",
    "Why DAG Needed: 1. Most optimized plan 2. Minimizing Plan 3. Fault tolerance 4. RDD Recovery\n",
    "\n",
    "DAG Execution: \n",
    "Step1: DAG Creation: set of Transformation of rdd generated through Lazy Evoluation\n",
    "Step2: DAG optimization: Logical to Physical\n",
    "Step3: DAG Execution: \n",
    "Action Triggering\n",
    "Job and Stage Creation: Stages breaks into Tasks\n",
    "Task scheduling and Execution: DAG Scheduler to Task Scheduler\n",
    "Result contain.\n",
    "\n",
    "-------------------------------------------------------------------------------------------\n",
    "Lineage Graph: Lineage Graph is a historical record of transformations, tracing back to the original data. \n",
    "               It is the representation of dependencies in between RDDs rather than the actual data.\n",
    "\n",
    "Use:\n",
    "1.  It's the basis for Spark's lazy evaluation strategy, only executing transformations when an action is invoked.\n",
    "2. Allows Spark to recover lost data by re-computing it from the lineage graph.\n",
    "\n",
    "Difference:\n",
    "\n",
    "Representation:\n",
    "DAG: dependencies between tasks or events\n",
    "Lineage: history of data transformations or processing steps.\n",
    "\n",
    "Cycle:\n",
    "A DAG does not contain cycles, while a lineage graph can contain cycles.\n",
    "Because dag represent dependencies between task that must be executed in particular order while lineage Represent history of data transformations that may be \n",
    "repeated or looped over.\n",
    "\n",
    "Direction:\n",
    "DAG: direction of edges represent the flow of dependencies between tasks\n",
    "Lineage: the direction of edges represent the flow of data transformations or processing steps.\n",
    "\n",
    "Use:\n",
    "DAG: used in task scheduling, workflow management, and distributed computing\n",
    "Lineage: used in data lineage and data quality analysis.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Spark handle fault tolerance:\n",
    "\n",
    "RDDs (Resilient Distributed Datasets):  When node fails, spark can reconstruct lost RDD partitions using lineage information, which represents the sequence\n",
    "of transformations applied to create the RDD. The lineage allow spark to recompute the lost data without needing to store the entire dataset.\n",
    "\n",
    "Lineage and Transformations: spark maintains a DAG of transformations applied to RDD if a node fails, Spark can recompute only the lost partitions by tracing\n",
    "back the lineage graph this minimize the amount of data that needs to be recalculated.\n",
    "\n",
    "Data Replication: By default, data is replicated at least once across different nodes. This replication ensures that there are redundant copies of data \n",
    "available, reducing the risk of data loss due to node failures.\n",
    "\n",
    "Checkpointing:  Checkpointing is a mechanism that allows you to save the state of an RDD to a reliable distributed file system like HDFS. This helps in \n",
    "faster recovery in case of failures since Spark can use the checkpointed data instead of recomputing the entire lineage.\n",
    "\n",
    "Task Re-execution: When a worker node fails, Spark can reschedule the failed tasks on other available nodes. The data needed for those tasks is \n",
    "recomputed using the lineage information.\n",
    "\n",
    "Persistent Storage: Intermediate data generated during transformations can be stored in memory or on disk. This allows Spark to use persisted data in \n",
    "case of node failures instead of recomputing it.\n",
    "\n",
    "Driver Recovery: If the driver node fails, the driver's state can be recovered by restarting the application and re-executing the driver code.\n",
    "\n",
    "c"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Difference between mapreduce and spark:\n",
    "1. Mapreduce has to read/write data to a disk while spark can do it in-memory make it faster.\n",
    "2. Hadoop can work or process far larger datasets than spark\n",
    "3. Hadoop MapReduce developed in java | Spark developed in scala.\n",
    "4. Fault tolerance is done through replication | done throgh rdd.\n",
    "5. Hard to work with real-time data | Easy\n",
    "6. Less costly comparison to spark | more costly."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Data Skewness: In scenario where some of the partitioned data has more data compared to others.\n",
    "How it Occurs: \n",
    "1. When data get re-partitioned\n",
    "2. Join/Group by operations\n",
    "\n",
    "Disadvantages:\n",
    "1. Impact on job performance\n",
    "2. Execution time increase then usual time\n",
    "3. Spark job resources not used in its full potential\n",
    "4. most resources become idle without doing any tasks\n",
    "5. distributed Processing got affected.\n",
    "6. Out of memory errors\n",
    "\n",
    "Fix Data Skewness:\n",
    "1. Broadcast Join: Instead of sort-merge join we use Broadcast join because 1. Shuffle 2. Sort 3. merge. Use a broadcast join for smaller datasets to \n",
    "   avoid shuffling large datasets.\n",
    "2. Salting Concept: Add a random value to the key to distribute the data more evenly across partitions.key1 == key1, it should also get satisfied by key1_<salt> = key1_<salt>\n",
    "3. Bucketing: In this, we group/bucket specific attributes of values into fixed-size so that data can evenly distribute.\n",
    "4. Custom Partioning: Implement a custom partitioner that distributes the data based on specific characteristics.\n",
    "5. AQE\n",
    "6. Repartition or Coalesce: Use repartition or coalesce to redistribute the data among partitions.\n",
    "\n",
    "Salting Work: When certain keys(hot keys) data have high number of occurences, which result in Skewness, now we add random number/string(salt) to the key\n",
    "so that the record with the same key is now spread accross multiple keys."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Catalyst Optimizer: It is a robust query optimization framework, responsible for transforming and optimizing Logical and Physical query plans.\n",
    "\n",
    "How It Works: \n",
    "1. Parsing: parsing the dataframe to create an AST(abstract syntax tree) represent logical structure of query.\n",
    "2. Analysis: Sementic Analysis perform on AST resolve/check column or table names, check for syntax errors and type checking to check query is valid or not.\n",
    "   Additionaly, collect metadata of columns and tables involved in it.\n",
    "3. Logical optimization: apply set of logical optimization, it simply or rewrite the query operations without changing overall behaviour or results.\n",
    "4. Logical Plan: Then it produce logical plan, it is like tree-like representation of queries operations and their dependencies. It captures high level \n",
    "operations like filter, joins, aggregation and projection.\n",
    "5. Physical Planing: based on logicla plan it generates many alternative physical plans, and explore different execution strategies and physical operators \n",
    "   for the specific data source involved in the query.\n",
    "6. Cost-Based Optimization: It checks each physical plan factors like, distribution, network latency, disk I/o, CPU Usage, and memory consumption after\n",
    "   that select one physical execution plan which cost is lowest estimated.\n",
    "7. Code-Generation: After that it generate efficient java bytes code or optimized sql code for executing the query which further optimized by underlying \n",
    "   execution engine.\n",
    "8. Execution: Finally spark sql execuets the optimized physical plan to get the desired results.\n",
    "\n",
    "Optimization Techniques:\n",
    "1. Predicate Pushdown: data need to filter out in source level before entering in the Processing pipeline, it push filter condition as close  to the data \n",
    "sources as possible.\n",
    "2. Column Purining / projection Pushdown: It reads only necessary columns and load it.\n",
    "3. Broadcast Join: In this small tables copy send to every worker nodes during join with large tables.\n",
    "4. Join ReOrdering: It re-orders the join's condition to minize the data Shuffle through that it enhance the parallelism and reduce overall execution time.\n",
    "5. Constant Folding: During query Analysis it evaluates Constant expressions through that reducing the computational overhead.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Lazy Evoluation: In spark it's a powerful concept that allows the optimization of data processing tasks by postponing the execution of transformations \n",
    "                 until an action is called.\n",
    "\n",
    "work:\n",
    "When we apply a transformations, spark records it but doesn't executes it immediately, meaning it builds dag of transformations and the execution occurs only\n",
    "when an action is triggered.\n",
    "\n",
    "Benefits:\n",
    "Optimization: Spark can optimize the entire computation plan by evaluating only the required transformations, skipping unnecessary work.\n",
    "\n",
    "Fault tolerance: Lazy evaluation allows Spark to recompute only the lost or incomplete data, ensuring fault tolerance.\n",
    "\n",
    "Performance: It enables pipelining of tasks, reducing the need to write intermediate results to disk.\n",
    "\n",
    "What if Mapreduce has lazy evoluation: \n",
    "1. On-demand processing 2. Resource Optimization 3.Pipeline Fusion 4. Reducing Redundant computations\n",
    "\n",
    "What if spark does not have lazy evoluation:\n",
    "Without lazy evaluation, Spark would need to execute transformations immediately, leading to unnecessary computations and increased memory usage. \n",
    "This could result in performance degradation, especially for complex data processing tasks involving multiple transformations."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Shared Variables: There are two different types of Shared Variables in spark -Broadcast Variable and Accumulator\n",
    "\n",
    "1. Broadcast Variables: are read-only Variables distributed across worker nodes in-memory. The data Broadcasted this way is cached in serialized form and\n",
    "                        deserialized before running each task. Used for cache a value in memory on all nodes generally small datasets only Broadcasted.\n",
    "\n",
    "2. Accumulator: Which are used to update the variables in parallel during execution/runtime and share results from worker to driver.similar to counters \n",
    "in Mapreduce. used for performing associative and commutative operations such as counters or sums."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "difference between Persist and Cache: are optimization techniques for both iterative and interactive Spark applications to improve the performance of the jobs or applications.\n",
    "iterative -> Reuse intermediate results\n",
    "interactive  -> allowing a two-way flow of information\n",
    "\n",
    "cache() -> MEMORY_ONLY\n",
    "Persist(level) -> MEMORY_ONLY, MEMORY_AND_DISK, MEMORY_ONLY_SER, MEMORY_AND_DISK_SER, DISK_ONLY (disk, or off-heap memory)\n",
    "\n",
    "unpersist() can be used to Freeing up space from the Storage memory.\n",
    "\n",
    "\n",
    "Checkpoint: used for fault-tolerance and to cut down the lineage of RDDs/dataframes especially in long and complex computations. It breaks the lineage and \n",
    "stores data to a reliable Storage(HDFS), used in scenario where lineage graph is too long or when you want to recover from failures efficiently.\n",
    "\n",
    "Note: use cache() -> for optimization purpose Checkpoint() -> for fault tolerance purpose, and reduce the lineage length in complex computations.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Deployment Mode:\n",
    "1. Local Mode: Execution not done in distributed manner, means single JVM process is used to produce both driver and executor.\n",
    "2. Client Mode: Driver is present in Client machine, means driver is not the part of cluster and on the other side executors run within the cluster.\n",
    "3. Cluster Mode: Driver and executor both run inside the cluster. Spark job submitted from local to cluster machine "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "coalesce and repartition: \n",
    "\n",
    "coalesce: is used to decrease the number of partitions without invoking Shuffling. It used in when output partitions is less than the input.\n",
    "repartition: helps to increase or decrease the number of partitions by doing Shuffling of data."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Resources Allocation in Spark:\n",
    "\n",
    "1.Static Resource Allocation:Resources will be fixed while running the spark application even if resources are free after some filter /transformation.\n",
    "\n",
    "2. Dynamic Resource Allocation: allows Spark applications to request and release resources (memory and CPU) from the cluster manager dynamically as per need.\n",
    "\n",
    "   How It Works:\n",
    "    When enabled, Spark applications can acquire additional resources when the workload increases and release resources when the workload decreases.\n",
    "    The cluster manager monitors the resource usage and makes decisions about allocating or deallocating resources based on the application's needs.\n",
    "\n",
    "  Benefits:\n",
    "    Efficient Resource Utilization: Ensures that resources are allocated based on the actual demand of the application, avoiding over-provisioning or \n",
    "    under-provisioning.\n",
    "    Small cluster, bigger impact : Enables sharing resources among multiple applications running on the same cluster.\n",
    "\n",
    "  How to enable? spark-defaults.conf | spark.shuffle.service.enabled true\n",
    "    spark.dynamicAllocation.enabled true\n",
    "\n",
    "    spark.dynamicAllocation.minExecutors 1\n",
    "\n",
    "    spark.dynamicAllocation.maxExecutors 4\n",
    "\n",
    "    spark.dynamicAllocation.executorIdleTimeout 60s\n",
    "\n",
    "    spark.dynamicAllocation.schedulerBacklogTimeout 60s\n",
    "\n",
    " Use Cases:\n",
    "    Well-suited for scenarios where the workload varies over time, and the demand for resources is unpredictable.\n",
    "    Commonly used in shared environments where multiple Spark applications coexist on the same cluster.\n",
    "     \n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Resources Isolation achieved is spark:\n",
    "\n",
    "1. Utilize resource managers like YARN, Mesos, etc. These managers allocate independent resources for each Spark application, such as memory, CPU, etc., \n",
    "   to ensure there is no interference between different applications.\n",
    "\n",
    "2. Spark's built-in scheduler can dynamically allocate resources based on the application's requirements, ensuring that each application receives enough \n",
    "   resources and avoiding resource contention issues.\n",
    "\n",
    "3. Utilize Spark’s dynamic resource allocation feature. Spark can dynamically adjust resource allocation based on the needs of the application, \n",
    "   enabling dynamic isolation of resources to ensure each application receives sufficient resources."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Difference between Standalone, mesos and yarn cluster:\n",
    "\n",
    "1. Independent cluster setup without a resource manager. | Master-slave architecture with dynamic resource allocation. | Part of the Hadoop ecosystem, follows\n",
    "   master-slave architecture.\n",
    "\n",
    "2. Resources managed manually, suitable for smaller  development/testing environments. | Fine-grained resource sharing across multiple frameworks. | Primarily\n",
    "   optimized for running Hadoop MapReduce jobs.\n",
    "\n",
    "3. Limited scalability and resource optimization compared to yarn, mesos | Supports diverse workloads including Hadoop, Spark, containers, etc. | Integrates \n",
    "   tightly with Hadoop ecosystem tools and workflows.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
